#!/usr/bin/env -S uv run python
# -*- python -*-
import json
import hashlib
import yaml
import sys
import os
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
from plumbum import cli, local
from plumbum.cmd import find, mkdir, ln, rm, mktemp, unzip, date, env

# Logging function
def log(level, message):
    colors = {
        "INFO":  "\033[1;34m",
        "WARN":  "\033[1;33m",
        "ERROR": "\033[1;31m",
        "RESET": "\033[0m",
    }
    timestamp = date("+%Y-%m-%d %H:%M:%S").strip()
    print(f"{colors[level]}[{timestamp}] [{level}] {message}{colors['RESET']}", file=sys.stderr)

# Generate a CID (cumulative id)
def generate_cid(cli_version, creation_time, primary_language, sha):
    hash_input = f"{cli_version} {creation_time} {primary_language} {sha}".encode()
    return hashlib.sha256(hash_input).hexdigest()[:6]

# Expand environment variables in paths
def expand_path(path):
    return local.env.expand(path)

# Initialize SQLite database and create metadata table
def init_metadata_db(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS metadata (
            git_branch TEXT,
            git_commit_id TEXT,
            git_owner TEXT,
            git_repo TEXT,
            ingestion_datetime_utc TEXT,
            primary_language TEXT,
            result_url TEXT,
            tool_name TEXT,
            tool_version TEXT,
            projname TEXT,
            db_file_size INTEGER
        )
    ''')
    conn.commit()
    conn.close()

# Process a single db.zip file
def process_db_file(zip_path, db_collection_dir):
    temp_dir = mktemp("-d").strip()
    try:
        unzip("-o", "-q", zip_path, "*codeql-database.yml", "-d", temp_dir)

        # Locate the YAML file regardless of its depth
        yaml_files = list(local.path(temp_dir).walk(
            filter=lambda p: p.name == "codeql-database.yml"))
        if not yaml_files:
            log("WARN", f"No codeql-database.yml found in {zip_path}")
            return

        yaml_path = yaml_files[0]
        with yaml_path.open("r") as f:
            yaml_data = yaml.safe_load(f)

        primary_language       = yaml_data["primaryLanguage"]
        creation_metadata      = yaml_data["creationMetadata"]
        sha                    = creation_metadata["sha"]
        cli_version            = creation_metadata["cliVersion"]
        creation_time          = creation_metadata["creationTime"]
        source_location_prefix = local.path(yaml_data["sourceLocationPrefix"])
        repo                   = source_location_prefix.name
        owner                  = source_location_prefix.parent.name
        cid = generate_cid(cli_version, creation_time, primary_language, sha)
        new_db_fname           = f"{owner}-{repo}-ctsj-{cid}.zip"
        result_file             = f"{db_collection_dir}/{new_db_fname}"
        
        file_size = local.path(zip_path).stat().st_size
        
        metadata = {
            "git_branch"             : "HEAD",
            "git_commit_id"          : sha,
            "git_owner"              : owner,
            "git_repo"               : repo,
            "ingestion_datetime_utc" : str(creation_time),
            "primary_language"       : primary_language,
            "result_url"             : result_file,
            "tool_name"              : f"codeql-{primary_language}",
            "tool_version"           : cli_version,
            "projname"               : f"{owner}/{repo}",
            "db_file_size"           : file_size,
        }

        metadata_db = local.path(db_collection_dir) / "metadata.sql"
        
        # Initialize database if it doesn't exist
        if not metadata_db.exists():
            init_metadata_db(str(metadata_db))
        
        # Insert metadata into SQLite database
        conn = sqlite3.connect(str(metadata_db))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO metadata (
                git_branch, git_commit_id, git_owner, git_repo, ingestion_datetime_utc,
                primary_language, result_url, tool_name, tool_version, projname, db_file_size
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            metadata["git_branch"],
            metadata["git_commit_id"],
            metadata["git_owner"],
            metadata["git_repo"],
            metadata["ingestion_datetime_utc"],
            metadata["primary_language"],
            metadata["result_url"],
            metadata["tool_name"],
            metadata["tool_version"],
            metadata["projname"],
            metadata["db_file_size"]
        ))
        conn.commit()
        conn.close()

        copy_path = local.path(db_collection_dir) / new_db_fname
        if not copy_path.exists():
            ln("-s", zip_path, copy_path)

    except Exception as e:
        log("WARN", f"Error processing {zip_path}: {e}")
    finally:
        rm("-rf", temp_dir)

# Main application class
class HEPC(cli.Application):
    """
    HEPC processes db.zip files found in a starting directory,
    copies updated names in a collection directory,
    and adds metadata information to a SQLite database "metadata.sql" in the directory.
    """

    db_collection_dir = cli.SwitchAttr(
        "--db_collection_dir", str, mandatory=True,
        help="Specify the database collection directory"
    )
    starting_path = cli.SwitchAttr(
        "--starting_path", str, mandatory=True, help="Specify the starting path"
    )
    max_dbs = cli.SwitchAttr(
        "--max_dbs", int, mandatory=False, default=100,
        help="Specify the maximum number of databases to ingest"
    )
    max_workers = cli.SwitchAttr(
        "--max_workers", int, mandatory=False, default=4,
        help="Specify the number of parallel workers for processing"
    )

    def main(self):
        db_collection_dir = expand_path(self.db_collection_dir)
        starting_path = expand_path(self.starting_path)

        mkdir("-p", db_collection_dir)
        log("INFO", f"Searching for db.zip files in {starting_path}")

        db_files = find(starting_path, "-type", "f", "-name", "db.zip",
                        "-size", "+0c").splitlines()

        if not db_files:
            log("WARN", "No db.zip files found in the specified starting path.")
            return

        db_files_to_process = db_files[0:self.max_dbs]
        log("INFO", f"Processing {len(db_files_to_process)} db.zip files with {self.max_workers} workers")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(process_db_file, zip_path, db_collection_dir): zip_path
                for zip_path in db_files_to_process
            }
            
            for future in as_completed(futures):
                zip_path = futures[future]
                try:
                    future.result()
                except Exception as e:
                    log("ERROR", f"Failed to process {zip_path}: {e}")

        log("INFO", "Processing completed.")

if __name__ == "__main__":
    HEPC.run()
